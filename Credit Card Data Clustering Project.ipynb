{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customers Segmentation based on their Credit Card usage behaviour\n",
    "\n",
    "Dataset for this notebook consists of credit card usage behavior of customers with 18 behavioral features. Segmentation of customers can be used to define marketing strategies.\n",
    "\n",
    "\n",
    "**Content of this Kernel:**\n",
    "\n",
    "* Data Preprocessing\n",
    "* Clustering using KMeans\n",
    "* Interpretation of Clusters\n",
    "* Visualization of Clusters using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"CC GENERAL.csv\")\n",
    "data=df\n",
    "print(df.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive Statistics of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer ID is unnecessary for building a clustering model (or any prediction model).\n",
    "# Dropping customer Id column.\n",
    "data.drop(['CUST_ID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The easiest way to handle the columns with null values is either to drop those rows, or fill with mean value.\n",
    "# Filling with mean value\n",
    "data = data.fillna(data.mean())\n",
    "\n",
    "# Verifying all the columns are filled\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-mean clustering model uses \"distances\" between data points to put them into groups\n",
    "# Logically, for it to work well, we will need to standardize the unit of the \"distances\"\n",
    "# Therefore, we would like to scale the dataset. Here, again, we would use the one of the most commonly used method - StandardScaler.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(data)\n",
    "data_scaled = scaler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Let's start with 4 clusters\n",
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model with different metrics\n",
    "# Decreasing the WCSS is the key objective\n",
    "# Silhouette coefficient should be nearer to +1\n",
    "# Lower the value of Davies-Bouldin Index would improve the performance.\n",
    "\n",
    "from sklearn.metrics import silhouette_score,calinski_harabasz_score,davies_bouldin_score\n",
    "\n",
    "labels = kmeans.fit_predict(data)\n",
    "\n",
    "print(\"Silhouette Coefficient: %0.3f\" % silhouette_score(data, labels))\n",
    "print(\"Calinski-Harabasz Index: %0.3f\" % calinski_harabasz_score(data, labels))\n",
    "print(\"Davies-Bouldin Index: %0.3f\" % davies_bouldin_score(data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method with WCSS (Within-Cluster Sum of Square).\n",
    "\n",
    "# To do that, we would need to build multiple models with varying \"number of clusters (i.e. K)\"\n",
    "# We can extract the wcss value from the model by using the inertia_ field\n",
    "# We would then put value of WCSS in each of model into an array and visualize them later\n",
    "\n",
    "wcss = []\n",
    "\n",
    "for i in range(2, 16): \n",
    "    kmeans = KMeans(n_clusters = i, random_state = 0)\n",
    "    kmeans.fit(data) \n",
    "    wcss.append(kmeans.inertia_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After having the series of WCSS values, we can then plot it out, and see what is the optimal K value.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "kvalue = range(2, 16)\n",
    "plt.plot(kvalue, wcss, marker='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to plot other metrics in the same chart as well\n",
    "\n",
    "silhouette = []\n",
    "davies_bouldin = []\n",
    "\n",
    "for i in range(2, 16): \n",
    "    kmeans = KMeans(n_clusters = i, random_state = 0)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    silhouette.append(silhouette_score(data, labels))\n",
    "    davies_bouldin.append(davies_bouldin_score(data, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvalue = range(2, 16)\n",
    "plt.plot(kvalue, silhouette, marker='o')\n",
    "plt.plot(kvalue, davies_bouldin, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-numeric columns if any (e.g., 'CUST_ID')\n",
    "df_numeric = df.select_dtypes(include='number')\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr = df_numeric.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(16, 10))\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=0.5)\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT',\n",
    "        'PAYMENTS', 'MINIMUM_PAYMENTS']\n",
    "\n",
    "for c in columns:\n",
    "    \n",
    "    Range=c+'_RANGE'\n",
    "    data[Range]=0        \n",
    "    data.loc[((data[c]>0)&(data[c]<=500)),Range]=1\n",
    "    data.loc[((data[c]>500)&(data[c]<=1000)),Range]=2\n",
    "    data.loc[((data[c]>1000)&(data[c]<=3000)),Range]=3\n",
    "    data.loc[((data[c]>3000)&(data[c]<=5000)),Range]=4\n",
    "    data.loc[((data[c]>5000)&(data[c]<=10000)),Range]=5\n",
    "    data.loc[((data[c]>10000)),Range]=6\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['BALANCE_FREQUENCY', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'PURCHASES_INSTALLMENTS_FREQUENCY', \n",
    "         'CASH_ADVANCE_FREQUENCY', 'PRC_FULL_PAYMENT']\n",
    "\n",
    "for c in columns:\n",
    "    \n",
    "    Range=c+'_RANGE'\n",
    "    data[Range]=0\n",
    "    data.loc[((data[c]>0)&(data[c]<=0.1)),Range]=1\n",
    "    data.loc[((data[c]>0.1)&(data[c]<=0.2)),Range]=2\n",
    "    data.loc[((data[c]>0.2)&(data[c]<=0.3)),Range]=3\n",
    "    data.loc[((data[c]>0.3)&(data[c]<=0.4)),Range]=4\n",
    "    data.loc[((data[c]>0.4)&(data[c]<=0.5)),Range]=5\n",
    "    data.loc[((data[c]>0.5)&(data[c]<=0.6)),Range]=6\n",
    "    data.loc[((data[c]>0.6)&(data[c]<=0.7)),Range]=7\n",
    "    data.loc[((data[c]>0.7)&(data[c]<=0.8)),Range]=8\n",
    "    data.loc[((data[c]>0.8)&(data[c]<=0.9)),Range]=9\n",
    "    data.loc[((data[c]>0.9)&(data[c]<=1.0)),Range]=10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['PURCHASES_TRX', 'CASH_ADVANCE_TRX']  \n",
    "\n",
    "for c in columns:\n",
    "    \n",
    "    Range=c+'_RANGE'\n",
    "    data[Range]=0\n",
    "    data.loc[((data[c]>0)&(data[c]<=5)),Range]=1\n",
    "    data.loc[((data[c]>5)&(data[c]<=10)),Range]=2\n",
    "    data.loc[((data[c]>10)&(data[c]<=15)),Range]=3\n",
    "    data.loc[((data[c]>15)&(data[c]<=20)),Range]=4\n",
    "    data.loc[((data[c]>20)&(data[c]<=30)),Range]=5\n",
    "    data.loc[((data[c]>30)&(data[c]<=50)),Range]=6\n",
    "    data.loc[((data[c]>50)&(data[c]<=100)),Range]=7\n",
    "    data.loc[((data[c]>100)),Range]=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([ 'BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n",
    "       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n",
    "       'PURCHASES_FREQUENCY',  'ONEOFF_PURCHASES_FREQUENCY',\n",
    "       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n",
    "       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n",
    "       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT' ], axis=1, inplace=True)\n",
    "\n",
    "X= np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "X = scale.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters=30\n",
    "cost=[]\n",
    "for i in range(1,n_clusters):\n",
    "    kmean= KMeans(i)\n",
    "    kmean.fit(X)\n",
    "    cost.append(kmean.inertia_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cost, 'bx-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmean= KMeans(6)\n",
    "kmean.fit(X)\n",
    "labels=kmean.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters=pd.concat([data, pd.DataFrame({'cluster':labels})], axis=1)\n",
    "clusters.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clusters:\n",
    "    grid= sns.FacetGrid(clusters, col='cluster')\n",
    "    grid.map(plt.hist, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 1 - cosine_similarity(X)\n",
    "\n",
    "pca = PCA(2)\n",
    "pca.fit(dist)\n",
    "X_PCA = pca.transform(dist)\n",
    "X_PCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = X_PCA[:, 0], X_PCA[:, 1]\n",
    "\n",
    "colors = {0: 'red',\n",
    "          1: 'blue',\n",
    "          2: 'green', \n",
    "          3: 'yellow', \n",
    "          4: 'orange',  \n",
    "          5:'purple'}\n",
    "\n",
    "names = {0: 'who make all type of purchases', \n",
    "         1: 'more people with due payments', \n",
    "         2: 'who purchases mostly in installments', \n",
    "         3: 'who take more cash in advance', \n",
    "         4: 'who make expensive purchases',\n",
    "         5:'who don\\'t spend much money'}\n",
    "  \n",
    "df = pd.DataFrame({'x': x, 'y':y, 'label':labels}) \n",
    "groups = df.groupby('label')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 13)) \n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=5,\n",
    "            color=colors[name],label=names[name], mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(axis='x',which='both',bottom='off',top='off',labelbottom='off')\n",
    "    ax.tick_params(axis= 'y',which='both',left='off',top='off',labelleft='off')\n",
    "    \n",
    "ax.legend()\n",
    "ax.set_title(\"Customers Segmentation based on their Credit Card usage bhaviour.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May 18th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
